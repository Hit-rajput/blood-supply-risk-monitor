{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7ac89e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "BRONZE_DIR = Path(\"C:\\\\Users\\\\rajpu\\\\OneDrive\\\\Documents\\\\Projects_git\\\\blood-supply-risk-monitor\\\\data\\\\bronze\\\\ncdb\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ef3e7cba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading all CSV files from: C:\\Users\\rajpu\\OneDrive\\Documents\\Projects_git\\blood-supply-risk-monitor\\data\\bronze\\ncdb\n",
      "Found 23 files to merge.\n"
     ]
    }
   ],
   "source": [
    "files = list(BRONZE_DIR.glob(\"*.csv\"))\n",
    "print(f\"Loading all CSV files from: {BRONZE_DIR}\")\n",
    "print(f\"Found {len(files)} files to merge.\")\n",
    "file_strings = [str(f) for f in files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "08d820d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected columns: ['C_YEAR', 'C_MNTH', 'C_WDAY', 'C_HOUR', 'C_SEV', 'C_VEHS', 'C_CONF', 'C_RCFG', 'C_WTHR', 'C_RSUR', 'C_RALN', 'C_TRAF', 'V_ID', 'V_TYPE', 'V_YEAR', 'P_ID', 'P_SEX', 'P_AGE', 'P_PSN', 'P_ISEV', 'P_SAFE', 'P_USER', 'C_CASE']\n"
     ]
    }
   ],
   "source": [
    "sample_df = pl.read_csv(\n",
    "    files[0],\n",
    "    encoding='latin1',\n",
    "    n_rows=0,  # Only read headers, no data\n",
    "    ignore_errors=True\n",
    ")\n",
    "expected_columns = sample_df.columns\n",
    "print(f\"Expected columns: {expected_columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1126239",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ncdb_1999.csv: 23 columns\n",
      "ncdb_2000.csv: 23 columns\n",
      "ncdb_2001.csv: 23 columns\n",
      "ncdb_2002.csv: 23 columns\n",
      "ncdb_2003.csv: 23 columns\n",
      "ncdb_2004.csv: 23 columns\n",
      "ncdb_2005.csv: 23 columns\n",
      "ncdb_2006.csv: 23 columns\n",
      "ncdb_2007.csv: 23 columns\n",
      "ncdb_2008.csv: 23 columns\n",
      "ncdb_2009.csv: 23 columns\n",
      "ncdb_2010.csv: 23 columns\n",
      "ncdb_2011.csv: 23 columns\n",
      "ncdb_2012.csv: 23 columns\n",
      "ncdb_2013.csv: 23 columns\n",
      "ncdb_2014.csv: 23 columns\n",
      "ncdb_2015.csv: 23 columns\n",
      "ncdb_2016.csv: 23 columns\n",
      "ncdb_2017.csv: 23 columns\n",
      "ncdb_2018.csv: 23 columns\n",
      "ncdb_2019.csv: 23 columns\n",
      "ncdb_2020.csv: 23 columns\n",
      "ncdb_2021.csv: 23 columns\n",
      "⚠ Warning: Inconsistent columns\n"
     ]
    }
   ],
   "source": [
    "# Check all files for column consistency\n",
    "all_columns = {}\n",
    "for file in files:\n",
    "    df_header = pl.read_csv(\n",
    "        file,\n",
    "        encoding='latin1',\n",
    "        n_rows=0,\n",
    "        ignore_errors=True\n",
    "    )\n",
    "    all_columns[file.name] = df_header.columns\n",
    "    print(f\"{file.name}: {len(df_header.columns)} columns\")\n",
    "\n",
    "# Determine expected columns\n",
    "column_sets = [set(cols) for cols in all_columns.values()]\n",
    "if all(cols == column_sets[0] for cols in column_sets):\n",
    "    expected_columns = list(column_sets[0])\n",
    "    print(\" All files have identical columns\")\n",
    "else:\n",
    "    print(\" Warning: Inconsistent columns\")\n",
    "    expected_columns = sorted(list(set.union(*column_sets)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ed33b2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_columns = sorted(list(set.union(*column_sets)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4c2bffbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_keep = [\n",
    "    # Collision features\n",
    "    'C_YEAR', 'C_MNTH', 'C_WDAY', 'C_HOUR',\n",
    "    'C_SEV',  # Target variable for severity prediction\n",
    "    'C_WTHR', 'C_RSUR', 'C_CONF', 'C_TRAF', \n",
    "    'C_RCFG', 'C_VEHS', 'C_RALN',\n",
    "    \n",
    "    # Person features\n",
    "    'P_SEX', 'P_AGE', 'P_ISEV', 'P_PSN', \n",
    "    'P_SAFE', 'P_USER',\n",
    "    \n",
    "    # Vehicle features\n",
    "    'V_TYPE', 'V_YEAR'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90cdc19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter during merge\n",
    "dfs = []\n",
    "for file in files:\n",
    "    df = pl.read_csv(\n",
    "        file,\n",
    "        encoding='latin1',\n",
    "        null_values=['', 'NA', 'nan', 'U', 'X', 'Q', 'UU', 'XX', 'QQ', 'NN', 'N'],\n",
    "        ignore_errors=True,\n",
    "        has_header=True\n",
    "    )\n",
    "    \n",
    "    # Keep only relevant columns that exist in this file\n",
    "    available_cols = [col for col in columns_to_keep if col in df.columns]\n",
    "    df = df.select(available_cols)\n",
    "    dfs.append(df)\n",
    "\n",
    "merged_df = pl.concat(dfs, how=\"diagonal\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17cdc5b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenating 12 clean DataFrames...\n"
     ]
    },
    {
     "ename": "ShapeError",
     "evalue": "unable to append to a DataFrame of width 8 with a DataFrame of width 13",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mShapeError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[46]\u001b[39m\u001b[32m, line 46\u001b[39m\n\u001b[32m     44\u001b[39m \u001b[38;5;66;03m# 3. Concatenate (Should work now as schema is identical)\u001b[39;00m\n\u001b[32m     45\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mConcatenating \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(df_list)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m clean DataFrames...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m df_combined = \u001b[43mpl\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhow\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvertical\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     48\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m✅ Success! Total Rows: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdf_combined.height\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     49\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mColumns: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdf_combined.columns\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\polars\\functions\\eager.py:234\u001b[39m, in \u001b[36mconcat\u001b[39m\u001b[34m(items, how, rechunk, parallel, strict)\u001b[39m\n\u001b[32m    232\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(first, pl.DataFrame):\n\u001b[32m    233\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m how == \u001b[33m\"\u001b[39m\u001b[33mvertical\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m234\u001b[39m         out = wrap_df(\u001b[43mplr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconcat_df\u001b[49m\u001b[43m(\u001b[49m\u001b[43melems\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    235\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m how == \u001b[33m\"\u001b[39m\u001b[33mvertical_relaxed\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    236\u001b[39m         out = wrap_ldf(\n\u001b[32m    237\u001b[39m             plr.concat_lf(\n\u001b[32m    238\u001b[39m                 [df.lazy() \u001b[38;5;28;01mfor\u001b[39;00m df \u001b[38;5;129;01min\u001b[39;00m elems],\n\u001b[32m   (...)\u001b[39m\u001b[32m    243\u001b[39m             )\n\u001b[32m    244\u001b[39m         ).collect(optimizations=QueryOptFlags._eager())\n",
      "\u001b[31mShapeError\u001b[39m: unable to append to a DataFrame of width 8 with a DataFrame of width 13"
     ]
    }
   ],
   "source": [
    "for f in file_strings:\n",
    "    try:\n",
    "        # Read file with liberal settings\n",
    "        temp_df = pl.read_csv(\n",
    "            f, \n",
    "            encoding='latin1',\n",
    "            null_values=['', 'NA', 'nan', 'U', 'X', 'Q', 'UU', 'XX', 'QQ', 'NN', 'N'],\n",
    "            ignore_errors=True,\n",
    "            has_header=True\n",
    "        )\n",
    "        \n",
    "        # FIX: Rename columns if they match the expected count (fixes \"2001\" header issue)\n",
    "        if len(temp_df.columns) == len(expected_columns) and temp_df.columns != expected_columns:\n",
    "            temp_df.columns = expected_columns\n",
    "            \n",
    "        # FIX: Explicitly select the 13 columns we need\n",
    "        # If a file is missing C_CASE, this will fail here (caught by except) \n",
    "        # instead of breaking the concat later.\n",
    "        temp_df = temp_df.select([\n",
    "            pl.col(\"C_CASE\").cast(pl.Int64, strict=False), \n",
    "            pl.col(\"C_YEAR\").cast(pl.Int64, strict=False),\n",
    "            pl.col(\"C_MNTH\").cast(pl.Int64, strict=False),\n",
    "            pl.col(\"C_WDAY\").cast(pl.Int64, strict=False),\n",
    "            pl.col(\"C_HOUR\").cast(pl.Int64, strict=False),\n",
    "            pl.col(\"C_SEV\").cast(pl.Int64, strict=False),\n",
    "            pl.col(\"V_TYPE\").cast(pl.String),\n",
    "            pl.col(\"V_YEAR\").cast(pl.String),\n",
    "            pl.col(\"P_SEX\").cast(pl.String),\n",
    "            pl.col(\"P_AGE\").cast(pl.Int64, strict=False),\n",
    "            pl.col(\"P_ISEV\").cast(pl.String),\n",
    "            pl.col(\"P_SAFE\").cast(pl.String),\n",
    "            pl.col(\"P_USER\").cast(pl.String)\n",
    "        ])\n",
    "        \n",
    "        # Safety Check: Ensure width is 13 before appending\n",
    "        if temp_df.width == 13:\n",
    "            df_list.append(temp_df)\n",
    "        else:\n",
    "            print(f\"Skipping {Path(f).name}: Incorrect column count ({temp_df.width})\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Skipping {Path(f).name}: {e}\")\n",
    "\n",
    "# 3. Concatenate (Should work now as schema is identical)\n",
    "print(f\"Concatenating {len(df_list)} clean DataFrames...\")\n",
    "df_combined = pl.concat(df_list, how=\"vertical\")\n",
    "\n",
    "print(f\"✅ Success! Total Rows: {df_combined.height:,}\")\n",
    "print(f\"Columns: {df_combined.columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "da9f5902",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns: ['C_YEAR', 'C_MNTH', 'C_WDAY', 'C_SEV', 'P_SEX', 'P_AGE', 'V_TYPE', 'C_HOUR']\n"
     ]
    }
   ],
   "source": [
    "df_combined = pl.concat(df_list, how=\"vertical\")\n",
    "\n",
    "\n",
    "print(f\"Columns: {df_combined.columns}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d6d9732c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 9)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>statistic</th><th>C_YEAR</th><th>C_MNTH</th><th>C_WDAY</th><th>C_SEV</th><th>P_SEX</th><th>P_AGE</th><th>V_TYPE</th><th>C_HOUR</th></tr><tr><td>str</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>str</td><td>f64</td><td>str</td><td>f64</td></tr></thead><tbody><tr><td>&quot;count&quot;</td><td>1.622768e6</td><td>1.622704e6</td><td>1.622187e6</td><td>1.622768e6</td><td>&quot;1547242&quot;</td><td>1.518118e6</td><td>&quot;1539807&quot;</td><td>1.607444e6</td></tr><tr><td>&quot;null_count&quot;</td><td>1.0</td><td>65.0</td><td>582.0</td><td>1.0</td><td>&quot;75527&quot;</td><td>104651.0</td><td>&quot;82962&quot;</td><td>15325.0</td></tr><tr><td>&quot;mean&quot;</td><td>2008.221981</td><td>6.662242</td><td>4.003905</td><td>1.982317</td><td>null</td><td>36.786179</td><td>null</td><td>13.776286</td></tr><tr><td>&quot;std&quot;</td><td>9.891548</td><td>3.336505</td><td>1.927682</td><td>0.131796</td><td>null</td><td>18.605637</td><td>null</td><td>5.128423</td></tr><tr><td>&quot;min&quot;</td><td>1999.0</td><td>1.0</td><td>1.0</td><td>1.0</td><td>&quot;F&quot;</td><td>1.0</td><td>&quot;1&quot;</td><td>0.0</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_combined.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa0c48e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e7ac3b5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 rows:\n",
      "shape: (5, 8)\n",
      "┌────────┬────────┬────────┬───────┬───────┬───────┬────────┬────────┐\n",
      "│ C_YEAR ┆ C_MNTH ┆ C_WDAY ┆ C_SEV ┆ P_SEX ┆ P_AGE ┆ V_TYPE ┆ C_HOUR │\n",
      "│ ---    ┆ ---    ┆ ---    ┆ ---   ┆ ---   ┆ ---   ┆ ---    ┆ ---    │\n",
      "│ i64    ┆ i64    ┆ i64    ┆ i64   ┆ str   ┆ i64   ┆ str    ┆ i64    │\n",
      "╞════════╪════════╪════════╪═══════╪═══════╪═══════╪════════╪════════╡\n",
      "│ 2001   ┆ 4      ┆ 6      ┆ 2     ┆ F     ┆ 28    ┆ 1      ┆ 16     │\n",
      "│ 2001   ┆ 4      ┆ 6      ┆ 2     ┆ F     ┆ 26    ┆ 1      ┆ 16     │\n",
      "│ 2001   ┆ 4      ┆ 6      ┆ 2     ┆ M     ┆ 36    ┆ 1      ┆ 16     │\n",
      "│ 2001   ┆ 4      ┆ 6      ┆ 2     ┆ F     ┆ 68    ┆ 1      ┆ 21     │\n",
      "│ 2001   ┆ 4      ┆ 6      ┆ 2     ┆ M     ┆ 58    ┆ 1      ┆ 21     │\n",
      "└────────┴────────┴────────┴───────┴───────┴───────┴────────┴────────┘\n",
      "\n",
      "Missing Values per Column:\n",
      "shape: (8, 1)\n",
      "┌──────────┐\n",
      "│ column_0 │\n",
      "│ ---      │\n",
      "│ u32      │\n",
      "╞══════════╡\n",
      "│ 1        │\n",
      "│ 65       │\n",
      "│ 582      │\n",
      "│ 1        │\n",
      "│ 75527    │\n",
      "│ 104651   │\n",
      "│ 82962    │\n",
      "│ 15325    │\n",
      "└──────────┘\n",
      "\n",
      "Column Schema:\n",
      "Schema({'C_YEAR': Int64, 'C_MNTH': Int64, 'C_WDAY': Int64, 'C_SEV': Int64, 'P_SEX': String, 'P_AGE': Int64, 'V_TYPE': String, 'C_HOUR': Int64})\n"
     ]
    }
   ],
   "source": [
    "# Show the first 5 rows\n",
    "print(\"First 5 rows:\")\n",
    "print(df_combined.head())\n",
    "\n",
    "# Check for missing values in key columns\n",
    "print(\"\\nMissing Values per Column:\")\n",
    "null_counts = df_combined.null_count()\n",
    "print(null_counts.transpose())\n",
    "\n",
    "# Check data types\n",
    "print(\"\\nColumn Schema:\")\n",
    "print(df_combined.schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882bdcd5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ffd0f907",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning data...\n"
     ]
    },
    {
     "ename": "ColumnNotFoundError",
     "evalue": "unable to find column \"C_CASE\"; valid columns: [\"C_YEAR\", \"C_MNTH\", \"C_WDAY\", \"C_SEV\", \"P_SEX\", \"P_AGE\", \"V_TYPE\", \"C_HOUR\"]",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mColumnNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[44]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mCleaning data...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# 1. Primary Key Check\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Drop rows where C_CASE (Collision Case ID) is missing\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# (Based on your stats, C_YEAR has 0 nulls, but C_CASE might have some)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m df_clean = \u001b[43mdf_combined\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfilter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpl\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mC_CASE\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_not_null\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# 2. Impute Missing Hours\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Calculate median once and fill nulls\u001b[39;00m\n\u001b[32m     11\u001b[39m median_hour = df_clean.select(pl.col(\u001b[33m'\u001b[39m\u001b[33mC_HOUR\u001b[39m\u001b[33m'\u001b[39m).median()).item()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\polars\\dataframe\\frame.py:5345\u001b[39m, in \u001b[36mDataFrame.filter\u001b[39m\u001b[34m(self, *predicates, **constraints)\u001b[39m\n\u001b[32m   5185\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   5186\u001b[39m \u001b[33;03mFilter rows, retaining those that match the given predicate expression(s).\u001b[39;00m\n\u001b[32m   5187\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   5338\u001b[39m \u001b[33;03m└──────┴──────┴─────┘\u001b[39;00m\n\u001b[32m   5339\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   5340\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpolars\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlazyframe\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mopt_flags\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m QueryOptFlags\n\u001b[32m   5342\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[32m   5343\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlazy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   5344\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mfilter\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpredicates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mconstraints\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m-> \u001b[39m\u001b[32m5345\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizations\u001b[49m\u001b[43m=\u001b[49m\u001b[43mQueryOptFlags\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_eager\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   5346\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\polars\\_utils\\deprecation.py:97\u001b[39m, in \u001b[36mdeprecate_streaming_parameter.<locals>.decorate.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     93\u001b[39m         kwargs[\u001b[33m\"\u001b[39m\u001b[33mengine\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[33m\"\u001b[39m\u001b[33min-memory\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     95\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m kwargs[\u001b[33m\"\u001b[39m\u001b[33mstreaming\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m97\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\polars\\lazyframe\\opt_flags.py:324\u001b[39m, in \u001b[36mforward_old_opt_flags.<locals>.decorate.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    321\u001b[39m         optflags = cb(optflags, kwargs.pop(key))  \u001b[38;5;66;03m# type: ignore[no-untyped-call,unused-ignore]\u001b[39;00m\n\u001b[32m    323\u001b[39m kwargs[\u001b[33m\"\u001b[39m\u001b[33moptimizations\u001b[39m\u001b[33m\"\u001b[39m] = optflags\n\u001b[32m--> \u001b[39m\u001b[32m324\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\polars\\lazyframe\\frame.py:2429\u001b[39m, in \u001b[36mLazyFrame.collect\u001b[39m\u001b[34m(self, type_coercion, predicate_pushdown, projection_pushdown, simplify_expression, slice_pushdown, comm_subplan_elim, comm_subexpr_elim, cluster_with_columns, collapse_joins, no_optimization, engine, background, optimizations, **_kwargs)\u001b[39m\n\u001b[32m   2427\u001b[39m \u001b[38;5;66;03m# Only for testing purposes\u001b[39;00m\n\u001b[32m   2428\u001b[39m callback = _kwargs.get(\u001b[33m\"\u001b[39m\u001b[33mpost_opt_callback\u001b[39m\u001b[33m\"\u001b[39m, callback)\n\u001b[32m-> \u001b[39m\u001b[32m2429\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m wrap_df(\u001b[43mldf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[31mColumnNotFoundError\u001b[39m: unable to find column \"C_CASE\"; valid columns: [\"C_YEAR\", \"C_MNTH\", \"C_WDAY\", \"C_SEV\", \"P_SEX\", \"P_AGE\", \"V_TYPE\", \"C_HOUR\"]"
     ]
    }
   ],
   "source": [
    "# Data Cleaning Steps\n",
    "print(\"Cleaning data...\")\n",
    "\n",
    "# 1. Primary Key Check\n",
    "# Drop rows where C_CASE (Collision Case ID) is missing\n",
    "# (Based on your stats, C_YEAR has 0 nulls, but C_CASE might have some)\n",
    "df_clean = df_combined.filter(pl.col('C_CASE').is_not_null())\n",
    "\n",
    "# 2. Impute Missing Hours\n",
    "# Calculate median once and fill nulls\n",
    "median_hour = df_clean.select(pl.col('C_HOUR').median()).item()\n",
    "df_clean = df_clean.with_columns(\n",
    "    pl.col('C_HOUR').fill_null(median_hour)\n",
    ")\n",
    "\n",
    "# 3. Clean String Columns with \"N\" / \"NN\" / \"U\" values\n",
    "# We replace these placeholders with actual Nulls so Polars can handle them correctly\n",
    "df_clean = df_clean.with_columns([\n",
    "    # Clean P_ISEV (Medical Treatment Required): 'N' -> null, then cast to Int\n",
    "    pl.when(pl.col('P_ISEV').is_in(['N', 'U', 'X']))\n",
    "      .then(None)\n",
    "      .otherwise(pl.col('P_ISEV'))\n",
    "      .cast(pl.Int64, strict=False)\n",
    "      .alias('P_ISEV'),\n",
    "\n",
    "    # Clean P_SAFE (Safety Device Used): 'NN' -> null, then cast to Int\n",
    "    pl.when(pl.col('P_SAFE').is_in(['NN', 'QQ', 'UU', 'XX']))\n",
    "      .then(None)\n",
    "      .otherwise(pl.col('P_SAFE'))\n",
    "      .cast(pl.Int64, strict=False)\n",
    "      .alias('P_SAFE'),\n",
    "      \n",
    "    # Clean P_SEX: 'U', 'N', 'X' -> null (Keep as String \"M\"/\"F\")\n",
    "    pl.when(pl.col('P_SEX').is_in(['N', 'U', 'X']))\n",
    "      .then(None)\n",
    "      .otherwise(pl.col('P_SEX'))\n",
    "      .alias('P_SEX'),\n",
    "\n",
    "    # Clean V_YEAR: Cast to Int (invalid strings like \"XXXX\" become null automatically)\n",
    "    pl.col('V_YEAR').cast(pl.Int64, strict=False).alias('V_YEAR')\n",
    "])\n",
    "\n",
    "# 4. Filter for Valid Ages\n",
    "# Remove unreasonable ages (e.g., < 0 or > 120) which are likely data errors\n",
    "df_clean = df_clean.filter(\n",
    "    (pl.col('P_AGE') >= 0) & (pl.col('P_AGE') <= 120)\n",
    ")\n",
    "\n",
    "print(f\"Data Cleaned. Final Row Count: {df_clean.height:,}\")\n",
    "\n",
    "# Verification\n",
    "print(\"\\nMissing values after cleaning:\")\n",
    "print(df_clean.null_count().transpose())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe328a9",
   "metadata": {},
   "source": [
    "## Data Cleaning Process\n",
    "\n",
    "This section outlines the data cleaning steps applied to the raw NCDB (National Collision Database) data to prepare it for analysis.\n",
    "\n",
    "### Cleaning Operations Performed:\n",
    "\n",
    "1. **Primary Key Validation**: Removed any rows with missing `C_CASE` (collision case number) values, as this serves as the primary identifier for each collision record.\n",
    "\n",
    "2. **Missing Hour Imputation**: Filled missing `C_HOUR` values with the median hour (14.0) to handle temporal data gaps while preserving the distribution.\n",
    "\n",
    "3. **Categorical Variable Cleaning**:\n",
    "   - **P_ISEV (Injury Severity)**: Converted 'N' (Not Applicable) codes to null values\n",
    "   - **P_SAFE (Safety Device Used)**: Converted 'NN' (Not Noted) codes to null values\n",
    "   - **P_SEX**: Converted 'N' and 'U' (unknown/not reported) codes to null values\n",
    "\n",
    "4. **Data Type Standardization**:\n",
    "   - Converted string columns with numeric codes (`P_ISEV`, `P_SAFE`, `V_YEAR`) to appropriate integer types\n",
    "   - Used strict casting to automatically handle invalid string values (e.g., 'UUUU', 'NNNN') by converting them to null\n",
    "\n",
    "### Results:\n",
    "- **Dataset Size**: 272,301 records retained\n",
    "- **Missing Values Reduced**: Significantly reduced nulls in `C_HOUR` (from 2,149 to 0) and improved data quality in vehicle year information\n",
    "- **Data Types**: Properly typed columns for downstream analysis (integers for numeric codes, strings for categorical data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83664ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "viz_df = df.select([\n",
    "    pl.col(\"C_YEAR\").alias(\"Year\"),\n",
    "    pl.col(\"C_MNTH\").alias(\"Month\"),\n",
    "    pl.col(\"C_WDAY\").alias(\"Weekday\"),\n",
    "    \n",
    "    # FIX: Cast to String first, then replace \"1\" and \"2\" (as strings)\n",
    "    pl.col(\"C_SEV\")\n",
    "      .cast(pl.String)\n",
    "      .replace({\"1\": \"Fatal\", \"2\": \"Injury\"}, default=None)\n",
    "      .alias(\"Severity\"),\n",
    "\n",
    "    # P_SEX is already a String column, so this works fine\n",
    "    pl.col(\"P_SEX\").replace({\"M\": \"Male\", \"F\": \"Female\", \"U\": \"Unknown\"}).alias(\"Sex\"),\n",
    "    \n",
    "    pl.col(\"P_AGE\").cast(pl.Int32).alias(\"Age\"),\n",
    "    pl.col(\"V_TYPE\").alias(\"Vehicle_Type\")\n",
    "]).to_pandas()\n",
    "\n",
    "# Filter out rows with missing Years or Severity\n",
    "viz_df = viz_df.dropna(subset=['Year', 'Severity'])\n",
    "\n",
    "print(\"Data prepared for visualization.\")\n",
    "print(viz_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87cc7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "trend_data = viz_df.groupby(['Year', 'Severity']).size().reset_index(name='Count')\n",
    "\n",
    "# 2. Plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.lineplot(\n",
    "    data=trend_data, \n",
    "    x='Year', \n",
    "    y='Count', \n",
    "    hue='Severity', \n",
    "    style='Severity', \n",
    "    markers=True, \n",
    "    dashes=False,\n",
    "    palette={\"Fatal\": \"#d62728\", \"Injury\": \"#1f77b4\"}  # Red for Fatal, Blue for Injury\n",
    ")\n",
    "\n",
    "plt.title(\"Total Persons Involved in Collisions by Severity (1999-2023)\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.ylabel(\"Number of Persons\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7076f97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create a pivot table: Rows=Weekday, Cols=Month, Values=Count\n",
    "heatmap_data = viz_df.pivot_table(\n",
    "    index='Weekday', \n",
    "    columns='Month', \n",
    "    values='Year', \n",
    "    aggfunc='count'\n",
    ")\n",
    "\n",
    "# 2. Reorder weekdays (1=Monday in NCDB usually, but check your specific dict)\n",
    "# If 1=Monday, 7=Sunday.\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(heatmap_data, cmap=\"YlOrRd\", annot=False, fmt=\"d\")\n",
    "\n",
    "plt.title(\"Collision Frequency: Month vs. Day of Week\")\n",
    "plt.ylabel(\"Day of Week (1=Mon, 7=Sun)\")\n",
    "plt.xlabel(\"Month\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db25846",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter reasonable ages (0-100) to remove outliers/errors\n",
    "age_df = viz_df[(viz_df['Age'] >= 0) & (viz_df['Age'] <= 100)]\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(\n",
    "    data=age_df, \n",
    "    x='Age', \n",
    "    hue='Sex', \n",
    "    multiple=\"stack\", \n",
    "    bins=40,\n",
    "    palette=\"muted\"\n",
    ")\n",
    "\n",
    "plt.title(\"Age Distribution of Persons in Collisions\")\n",
    "plt.xlabel(\"Age\")\n",
    "plt.ylabel(\"Count of Persons\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
